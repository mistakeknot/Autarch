# Gurgeh Roadmap

> The best tool for vibecoders to create specs that produce great agent output

## Vision

Vibecoders want to describe what they want and have agents build it well. Gurgeh's job is to minimize the time between "I have an idea" and "my agent has a clear, testable brief." Every feature should either reduce user effort or increase output quality â€” ideally both.

The north star: **a spec that makes agents succeed on the first try.**

---

## Done

### M0: Foundation âœ…

**Goal:** Phase-based PRD sprint with propose-first UX

| Task | Status | Notes |
|------|--------|-------|
| 8-phase arbiter sprint | âœ… | Vision â†’ Acceptance Criteria |
| Draft generation with alternatives | âœ… | 2-3 options per phase |
| Consistency checking | âœ… | 4 checkers: user-feature, goal-feature, scope-creep, assumption |
| Confidence scoring | âœ… | Two calculators: fast (arbiter) + content-aware (gurgeh) |
| Spec storage + YAML persistence | âœ… | .gurgeh/specs/, sprint save/load |
| TUI sprint view | âœ… | Bubble Tea, keyboard nav, research panel |
| Spec validation (hard/soft) | âœ… | Required fields, CUJ validation, status checks |

### M1: Research Integration âœ…

**Goal:** Pollard findings inform spec generation

| Task | Status | Notes |
|------|--------|-------|
| Intermute research provider | âœ… | Create/link specs, fetch findings |
| Phase-specific research config | âœ… | Visionâ†’github-scout, Problemâ†’arxiv, Featuresâ†’competitor-tracker |
| Deep scan async plumbing | âœ… | Start/check/import lifecycle |
| Research quality in confidence | âœ… | 0.3Ã—count + 0.3Ã—diversity + 0.4Ã—relevance |
| Vision alignment | âœ… | Load vision spec, cross-check PRD sections |

### M2: Thinking Shapes âœ…

**Goal:** Metacognitive preambles force quality-standard formulation before generation

| Task | Status | Notes |
|------|--------|-------|
| 5 shapes: Deductive, Inductive, Abductive, Contrapositive, DSL | âœ… | pkg/thinking/ |
| Phase-to-shape defaults | âœ… | Each phase gets the right thinking strategy |
| Per-sprint shape overrides | âœ… | SprintState.ShapeOverrides |
| Shape-aware confidence | âœ… | Deductive/DSL â†’ +specificity, Contrapositive â†’ +assumptions |
| Pollard agent hunter integration | âœ… | Contrapositive for competitive, Abductive for general |

---

## Planned

### M3: Agent-Ready Brief Decomposition

**Goal:** Transform monolithic specs into focused, context-window-sized briefs that agents actually consume well

**Why first:** This is the closest to the user's actual goal. A perfect spec that produces a bad agent brief is a perfect failure.

| Task | Status | Notes |
|------|--------|-------|
| Task-level brief extraction | â¬œ | Decompose spec into independent work units with focused context |
| Context budget system | â¬œ | Size briefs for agent context windows (2K-4K tokens per task) |
| Acceptance criteria â†’ test stubs | â¬œ | Generate skeleton test files from Given/When/Then requirements |
| Dependency graph per brief | â¬œ | Each brief knows what it depends on and what depends on it |
| Brief quality scoring | â¬œ | Is this brief specific enough for an agent to act on without asking questions? |

### M4: Self-Critique Loop

**Goal:** Agents evaluate their own drafts against shape criteria before proposing to the user

**Why second:** Better first drafts = less user effort. The user should rarely need to revise.

| Task | Status | Notes |
|------|--------|-------|
| Generate â†’ evaluate â†’ revise pipeline | â¬œ | One self-critique pass before proposing |
| Shape-specific evaluation rubrics | â¬œ | Deductive: "did it state criteria first?" Contrapositive: "did it enumerate failures?" |
| Critique-to-revision mapping | â¬œ | Failed rubric items become revision instructions |
| Critique visibility in TUI | â¬œ | Show what was caught and fixed (builds trust) |
| Configurable critique depth | â¬œ | 0 = off (current behavior), 1 = single pass, 2 = thorough |

### M5: Shape Output Validation

**Goal:** Verify that thinking shape preambles actually improved output quality

**Why third:** Without validation, thinking shapes are cargo cult prompting.

> **Note:** `internal/gurgeh/validation/` exists but validates *stakeholder alignment* (Broker pattern for Product/Design/Engineering perspectives), not shape output compliance. Shape-specific validators are a different system.

| Task | Status | Notes |
|------|--------|-------|
| Per-shape output validators | â¬œ | Check structural compliance (e.g., DSL output has schema fields) |
| Validation score per section | â¬œ | 0-1 "did the shape help?" metric |
| Validator feedback â†’ re-generation | â¬œ | If shape wasn't followed, re-prompt with explicit correction |
| Shape effectiveness tracking | â¬œ | Over time, learn which shapes work best for which project types |

### M6: Subagent Enrichment Passes

**Goal:** The 8 subagent types automatically critique and enrich drafts during the sprint

**Why fourth:** Moves from "user catches problems" to "system catches problems."

> **Existing:** 8 subagent modules are individually implemented (Strategist, Navigator, Sentinel, Recognizer, Prophet, Scribe, Tracer, Broker) but not orchestrated into the sprint flow. The gap is the dispatch pipeline, not the agents themselves.

| Task | Status | Notes |
|------|--------|-------|
| Phase â†’ subagent mapping | â¬œ | Which subagents run on which phases (dispatch registry) |
| Enrichment pass pipeline | â¬œ | Draft â†’ subagent critique â†’ merge suggestions â†’ propose to user |
| Strategist: architecture implications | âœ… | Implemented in architecture/strategist.go; needs sprint wiring |
| Navigator: journey completeness | âœ… | Implemented in navigator/navigator.go; needs sprint wiring |
| Sentinel: security surface analysis | âœ… | Implemented in nfr/nfr.go (STRIDE); needs sprint wiring |
| Recognizer: anti-pattern detection | âœ… | Implemented in patterns/recognizer.go; needs sprint wiring |
| Prophet: performance prediction | âœ… | Implemented in performance/prophet.go; needs sprint wiring |
| Scribe: contract generation | âœ… | Implemented in contracts/contracts.go; needs sprint wiring |
| Tracer: dependency analysis | âœ… | Implemented in dependency/tracer.go; needs sprint wiring |
| Broker: stakeholder alignment | âœ… | Implemented in validation/validation.go; needs sprint wiring |
| Configurable subagent depth | â¬œ | none / quick / thorough |

### M7: Research-Annotated Drafts

**Goal:** Proactive intelligence â€” research findings annotate drafts inline, not in a side panel

**Why fifth:** Vibecoders won't check a research panel. Findings must appear where they're relevant.

| Task | Status | Notes |
|------|--------|-------|
| Finding â†’ section relevance matching | â¬œ | Link findings to the specific section they inform |
| Inline annotations in draft content | â¬œ | "âš¡ Competitor X already ships this as..." |
| Conflict detection: assumption vs finding | â¬œ | "Your assumption A conflicts with finding F" |
| Auto-import research on phase advance | â¬œ | No manual polling; findings flow in as phases progress |

### M8: Hypothesis Lifecycle

**Goal:** Close the spec â†’ implementation â†’ validation feedback loop

**Why sixth:** Without this, specs are write-once documents that rot. Hypotheses must be trackable.

> **Existing:** Hypothesis struct in `specs/schema.go` has Status/Metric/Target fields. `specs/evolution.go` has `CheckAssumptionDecay()` with DecayDays logic. The schema is ready; what's missing is the state machine, automation, and signal emission.

| Task | Status | Notes |
|------|--------|-------|
| Hypothesis status tracking | ðŸ”¶ | Schema exists (untested/validated/invalidated); needs state machine + transitions |
| Link hypotheses to metrics | ðŸ”¶ | Metric/Target fields exist; needs connection to actual measurement |
| Invalidation â†’ spec revision trigger | â¬œ | Failed hypothesis flags affected sections for review |
| Assumption decay automation | ðŸ”¶ | CheckAssumptionDecay() exists; needs background scheduling + signal emission |

### M9: Spec Versioning & Diff

**Goal:** Multi-session iteration with full history

**Why seventh:** Vibecoders iterate across sessions. They need to see what changed and why.

> **Existing:** `specs/evolution.go` has `SaveRevision()` creating `{spec_id}_v{N}.yaml` snapshots with revision metadata. `diff/diff.go` has `DiffSpecs()` returning per-section changes. The backend works; the TUI and user-facing annotation are missing.

| Task | Status | Notes |
|------|--------|-------|
| Version snapshots on save | âœ… | evolution.go SaveRevision() â†’ .gurgeh/specs/history/ |
| Structured diff between versions | âœ… | diff.go DiffSpecs() â€” per-section change summary |
| Side-by-side TUI comparison | â¬œ | View two versions simultaneously |
| Change reason tracking | ðŸ”¶ | Trigger field exists (manual/signal:competitive/etc.); needs detailed user annotations |

### M10: Agent-Native Export

**Goal:** Meet vibecoders where their agents live

**Why last:** Only valuable once the specs themselves are excellent.

| Task | Status | Notes |
|------|--------|-------|
| Export to Claude Projects format | â¬œ | Project instructions + task files |
| Export to Cursor rules | â¬œ | .cursorrules with spec context |
| Export to Codex instructions | â¬œ | AGENTS.md-compatible format |
| Export to CLAUDE.md | â¬œ | Project-level context for Claude Code |
| Spec templates by domain | â¬œ | SaaS, CLI tool, mobile app, API service starters |

---

## Principles

1. **Less user effort, better output.** Every feature must pass this test.
2. **Agents are the audience.** The spec exists to make agents succeed, not to satisfy a process.
3. **Proactive over passive.** Don't make users check panels â€” bring insights to where they're working.
4. **Validate, don't assume.** Thinking shapes, subagent passes, and self-critique must prove they help.
5. **Context windows are real.** Briefs must be sized for how agents actually consume instructions.
6. **Specs are living documents.** Versioning, decay, and feedback loops keep them honest.
